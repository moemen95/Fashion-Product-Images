{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from classifiers import get_pretrained_model, create_attributes_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_conv_model, pretrained_fc, fc_dim = get_pretrained_model(\"vgg16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_conv_model, resnet_fc, resnet_fc_dim = get_pretrained_model(\"vgg16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import image_loader\n",
    "from torchvision import transforms\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = image_loader(image_name=\"data/ClothingAttributeDataset/train/000001.jpg\", transforms=None, use_gpu=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transforms = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Scale(266),\n",
    "    transforms.CenterCrop((400, 266)),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "to_image = transforms.Compose([\n",
    "    \n",
    "        transforms.ToPILImage()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_url = 'data/ClothingAttributeDataset/train/000002.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = image_loader(image_name=image_url, transforms=data_transforms, use_gpu=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 400, 266])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.squeeze(dim=0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 400, 266])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = to_image(x.squeeze(dim=0).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg_conv_model, _, _ = get_pretrained_model(\"alexnet\", pop_last_pool_layer=True)\n",
    "# vgg_conv_model = nn.Sequential(*list(pretrained_conv_model.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential (\n",
       "  (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "  (1): ReLU (inplace)\n",
       "  (2): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "  (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (4): ReLU (inplace)\n",
       "  (5): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
       "  (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (7): ReLU (inplace)\n",
       "  (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): ReLU (inplace)\n",
       "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU (inplace)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_conv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_features = vgg_conv_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 24, 15])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AttributeFCN(nn.Module):\n",
    "    def __init__(self, in_channels, out_dims):\n",
    "        super().__init__()\n",
    "        model_steps = [\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Conv2d(in_channels, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, out_dims, 1)\n",
    "        ]\n",
    "        self.model = nn.Sequential(*model_steps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get Conv Layer output.  Output channels = number of classes\n",
    "        classes_conv_out = self.model(x)\n",
    "        \n",
    "        # Do Global Average Pooling on the Conv Layer with Number of Channels = Classes\n",
    "        pool_size = (classes_conv_out.size(2), classes_conv_out.size(3))\n",
    "        verage_pool = F.avg_pool2d(classes_conv_out, pool_size)\n",
    "        average_pool_flatten = average_pool.view(average_pool.size(0), -1)\n",
    "        classes_softmax = F.softmax(average_pool_flatten)\n",
    "        \n",
    "        return classes_conv_out, classes_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = AttributeFCN(512, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes_conv_out, classes_p = model(out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 25, 16])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_conv_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  0.3602\n",
       " [torch.FloatTensor of size 1x1], \n",
       "  2\n",
       " [torch.LongTensor of size 1x1])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.max(classes_p.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 16])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_conv_out[0, 1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18e6ab470>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAADfCAYAAAD4Bhh5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGbBJREFUeJzt3X2QVfWd5/H3h6abhgYFBNoWUFAhiq7BDMuYNcn4kChx\nx2BqdiyyNVmq1l3yh5NJZqd2Sp2qmcwfbGW3JslsbVUyRYwrs5PEoUYdKdedFLJmkuw6IBpAeRIU\nELB5lAebh6YfvvvHPYx3CHRf6O/d7j71eVV19enfPfdzf7++t799+vTv/o4iAjMzK68Rg90BMzOr\nLxd6M7OSc6E3Mys5F3ozs5JzoTczKzkXejOzkqtboZe0QNI2STskPVavxzEzs76pHvPoJTUAbwOf\nA/YCrwFfiojN6Q9mZmZ9qtcR/XxgR0S8GxFngWeAhXV6LDMz68PIOuVOBfZUfb0X+PWL7dw8vjla\n2samdqDjVHNqHgDKj6QOb0xWTz06mi9G5g9+ZFNPal53d0NqXkUdnvSu/GO2EV3pkag3P7O3MT+z\nHqIx/3k/u3vf4YiY3N9+9Sr0/ZK0BFgCMObqFj7/dO4B/8833JSaB0BDPX5A84ty4/E6FKc6/O7o\nmtCdntk6/Whq3sHDV6TmAUh1OF16YFR65pj2/F8ejR35Yz85NT2SqMPr/Wxb/m/O9x55bHct+9Xr\n1M0+YHrV19OKtn8UEcsiYl5EzGseX4ejbzMzA+pX6F8DZkmaKakJWASsrNNjmZlZH+py6iYiuiX9\nLvAToAF4KiI21eOxzMysb3U7Rx8RLwEv1SvfzMxq43fGmpmV3KDNuqnWcaaZn2+dlZo5etKp1DyA\nxpG5U/cAOjeOT8/suiJ/ZkPDqeExZfPkK1NS865+L38+4P5P1+H56c5/fjpuO5OeGT35x5Y7FzyZ\nnnn9qn+bnvnJG3emZ75X434+ojczKzkXejOzknOhNzMrORd6M7OSc6E3Mys5F3ozs5JzoTczK7kh\nMY9+bPMZPn3T9tTMNw9ek5oHcOLE6PTMeqyw2tuSP9+/ZU9+T6e8kX+ccfi23Dnqx2bn9/Hqn+fP\nzT98e3okM/8yf25++x1N6Zl3rP9X6ZlXXdWRnrnxQH5NqpWP6M3MSs6F3sys5FzozcxKzoXezKzk\nXOjNzErOhd7MrORc6M3MSm5A8+gl7QI+BHqA7oiYJ2ki8NfADGAX8HBEHO0rp+PkaH6xds5AuvKr\nfZvYmZoH0LA3/yLmM589lp65+8EJ6ZmnPnE6PbOhsw7vSziRm7fgX7+aGwgc6ByXntm+eXZ65tkr\n899mo/y3EHDo7UnpmZNnH07PbBrZnZ5Zq4wj+rsjYm5EzCu+fgxYHRGzgNXF12ZmNkjqcepmIbC8\n2F4OPFSHxzAzsxoNtNAH8LKk1yUtKdpaI6K92N4PtA7wMczMbAAGehLuUxGxT9IUYJWkrdU3RkRI\nuuDiI8UvhiUADRPyzymbmVnFgI7oI2Jf8fkg8DwwHzggqQ2g+HzwIvddFhHzImJew9iWgXTDzMz6\ncNmFXlKLpHHntoH7gLeAlcDiYrfFwAsD7aSZmV2+gZy6aQWel3Qu50cR8XeSXgNWSHoE2A083F+Q\neqDxRO6SqHEqfypkb1PuErgAW393bHrmr8/Z2v9Ol+hMd/4yxVvark/PvOHPd6Tmvb7+E6l5APt+\nI3+p3qt2p0cSI/LnQra8n/8z1D02fznlDzZOTs8c/8/yp2zW6rILfUS8C3z8Au1HgHsH0ikzM8vj\nd8aamZWcC72ZWcm50JuZlZwLvZlZybnQm5mVnAu9mVnJ5a9DehmiKTg7tSs1s3F//rzvnpb8ecVq\nzM9c+8as9MyGk/nHBGPb8+c/b/+PN6bmNR/O76Mify75mN/en5457pGz6Zkxbkx65slr8ue8d43L\nf45On82vSbXyEb2ZWcm50JuZlZwLvZlZybnQm5mVnAu9mVnJudCbmZXckJheSa/Q6YbUyJGn86fF\nxbHcPgKM6M7PvO5/nkrP3Htv/sVhOutwYbERnbl5HTfmTvsFuPGG/KmQDX94RXrm6TlXpWf2jsz/\nuTwzJX+KMvndpGVU/nTVWvmI3sys5FzozcxKzoXezKzkXOjNzEqu30Iv6SlJByW9VdU2UdIqSduL\nzxOqbntc0g5J2yTdX6+Om5lZbWo5on8aWHBe22PA6oiYBawuvkbSHGARcEtxn+9Kyp9WYmZmNeu3\n0EfEz4APzmteCCwvtpcDD1W1PxMRnRGxE9gBzE/qq5mZXYbLnUffGhHtxfZ+oLXYngr8Q9V+e4u2\nXyFpCbAEYFTrOG646f3L7MqFjbgpf5nR939ybXrmhLe70zP33ZU/5/0/Lf7L9Mzf/+mX0jOvey43\nr2tc/h+k3c9dnZ55+I8/TM/kJ03pkSdm5c95/9jc99Izt2y/YNkakIOH8t/rUKsB/zM2IgK45Koa\nEcsiYl5EzGu6cvRAu2FmZhdxuYX+gKQ2gOLzwaJ9HzC9ar9pRZuZmQ2Syy30K4HFxfZi4IWq9kWS\nRkmaCcwC1g6si2ZmNhD9nqOX9GPgLmCSpL3AnwDfBFZIegTYDTwMEBGbJK0ANgPdwKMR0VOnvpuZ\nWQ36LfQRcbH/mN17kf2XAksH0ikzM8vjd8aamZXckFim+Gz3SHYfyl2zdsS2sal5AE35MyHZc/5b\n0RI8OO+19MwVB/95eubEdfkvvzGvbk7NO3Xn7NQ8gGM3NKZnHt9zZXomc/LPujYezT+2fLt9Snrm\nxLbj6ZnjR59Jz9xd434+ojczKzkXejOzknOhNzMrORd6M7OSc6E3Mys5F3ozs5JzoTczK7khMY8e\nYMSI3GWFO1vzJ72PPpj/7Wq44mx65nsnJ6Zn7nz+hvTME/Pzxz7q+M2peYcXnk7NAxj79/mvo2jI\nX5Z78rVH0zN7rlN65tGdue/BAbjnpg3pmc9vmZueWSsf0ZuZlZwLvZlZybnQm5mVnAu9mVnJudCb\nmZWcC72ZWcm50JuZlVwtlxJ8CvhN4GBE3Fq0fQP498ChYrcnIuKl4rbHgUeAHuD3IuIn/T1GdIvO\nI6MvawAXc/O3D6fmARy8qzU9s7Exf83vCaNOpWceXrA/PbPr/1ydnnky+SnqPtycGwiMzJ+aD/nT\n05nQnN/RD06PSc8c9UFDeuYL225Lz5wzrT09c2eN+9VyRP80cKHLY3wnIuYWH+eK/BxgEXBLcZ/v\nSsp/FszMrGb9FvqI+BnwQY15C4FnIqIzInYCO4D5A+ifmZkN0EDO0X9V0kZJT0k69x7kqcCeqn32\nFm2/QtISSeskrevpODmAbpiZWV8ut9B/D7gemAu0A9+61ICIWBYR8yJiXsPYlsvshpmZ9eeyCn1E\nHIiInojoBb7PR6dn9gHTq3adVrSZmdkguaxCL6mt6ssvAm8V2yuBRZJGSZoJzALWDqyLZmY2ELVM\nr/wxcBcwSdJe4E+AuyTNBQLYBXwFICI2SVoBbAa6gUcjot/5g83NXdx8097LHcMF7f3CjNQ8gN56\nLOr81rj0yJ8eyV2qF2BES1d+5s3500BHvp47fa9ld/6ksZ5R+UsKN9ZhiuGhk/mnVI+2X5GeOf2T\n+dMWR43MX+Z8RsuR9Mxa9Vu6IuJLF2j+QR/7LwWWDqRTZmaWx++MNTMrORd6M7OSc6E3Mys5F3oz\ns5JzoTczKzkXejOzkqvHzPBLNr7xFF9o3ZCa+eSx61LzAHq+cDQ9s+tMU3rmp6/dlZ75D7tnpGfO\n+G/5xxm7f68jNa93V/5c8mjIX1N49IH0SM4evyo9s3F8/nsIGq7vTc/cd/TK9MzjZ/KXvK6Vj+jN\nzErOhd7MrORc6M3MSs6F3sys5FzozcxKzoXezKzkhsT0ygOnr+DbGz6bmjmxM38a1+k1E9MzZ352\nd3rmun3Xpmc2rR+bnjni7IfpmY2/zF32ecz+/NfRhKf/b3rmzm9+Mj3z7nvWp2e+snpueuaJM6PS\nM8+czJ/23NCQPw20Vj6iNzMrORd6M7OSc6E3Myu5fgu9pOmSXpG0WdImSV8r2idKWiVpe/F5QtV9\nHpe0Q9I2SffXcwBmZta3Wo7ou4E/iIg5wB3Ao5LmAI8BqyNiFrC6+JritkXALcAC4LuS8i9oaWZm\nNem30EdEe0S8UWx/CGwBpgILgeXFbsuBh4rthcAzEdEZETuBHcD87I6bmVltLukcvaQZwO3AGqA1\nIs5dfn0/0FpsTwX2VN1tb9F2ftYSSeskres5cfISu21mZrWqeR69pLHAs8DXI+KE9NFSqxERki5p\nwnFELAOWAYyZ1RZjW85cyt371TEtf3nZCZ/Zn565bcc16ZnXzTiUntl7d/4v4/2n2tIzJ6/vSs3b\nP78xNQ8gXpydntm7rQ7L/17aj3RNbvoXO9Mz3zmcv5zyyKae9Mzmxu70zFrVdEQvqZFKkf9hRDxX\nNB+Q1Fbc3gYcLNr3AdOr7j6taDMzs0FQy6wbAT8AtkTEt6tuWgksLrYXAy9UtS+SNErSTGAWsDav\ny2ZmdilqOXVzJ/Bl4E1J594T/QTwTWCFpEeA3cDDABGxSdIKYDOVGTuPRkT+30FmZlaTfgt9RPwC\nuNi1z+69yH2WAksH0C8zM0vid8aamZWcC72ZWckNiWWKe0+PpGNz7hLAU+/Nn+jT0ng2PZNrj6RH\n/tpV76VndvbmTzN877dOp2eeeeLq1LyZPzqcmgewdfKU9MxpP89fAnfdxtvTMw/dVYefoe7849XP\n3rY5PfPv370xPbNWPqI3Mys5F3ozs5JzoTczKzkXejOzknOhNzMrORd6M7OSc6E3Myu5ITGPPpp6\n6b02d071ybNNqXkAp7vy55J/cfqG9MwJI/OXFP7b/XPTM99+v7X/nS7RiAebU/Na14xKzQOY9nL+\n8r9H5uT/KE/emLvkM8DYLfnfz9NT8t9DsKb92vRM6rDsc618RG9mVnIu9GZmJedCb2ZWci70ZmYl\n50JvZlZyLvRmZiVXyzVjp0t6RdJmSZskfa1o/4akfZLWFx8PVN3ncUk7JG2TdH89B2BmZn2rZfJt\nN/AHEfGGpHHA65JWFbd9JyL+rHpnSXOARcAtwDXAy5Jm93Xd2ObGbma3Hby8EVzEpOaO1DyAX+6f\nlp75F2t+Iz3z07e8nZ65Y//k9Myezob0zLvv3piat/qaj6XmAUQdxj02/ynnxLX5c/Nb3s+f8346\nf3l/GhvyL3M9ckz+2GvV7xF9RLRHxBvF9ofAFmBqH3dZCDwTEZ0RsRPYAczP6KyZmV26SzpHL2kG\ncDuwpmj6qqSNkp6SNKFomwrsqbrbXvr+xWBmZnVUc6GXNBZ4Fvh6RJwAvgdcD8wF2oFvXcoDS1oi\naZ2kdWeP519SzszMKmoq9JIaqRT5H0bEcwARcSAieiKiF/g+H52e2QdMr7r7tKLtn4iIZRExLyLm\nNV05eiBjMDOzPtQy60bAD4AtEfHtqva2qt2+CLxVbK8EFkkaJWkmMAtYm9dlMzO7FLX8W/1O4MvA\nm5LWF21PAF+SNBcIYBfwFYCI2CRpBbCZyoydR/uacWNmZvXVb6GPiF8AusBNL/Vxn6XA0lo7caaz\nkS3vXlPr7jWZPWN/ah7AmW1Xpmfq6rPpmR1d+UvB9vTkv7du/Bv5S0mv3fDx1Ly5v7U9NQ+guaE7\nPfO1D25Kzxz/Tv50wOP/5kR6ZmtzZ3rmkRMt6ZmNjYN3vOt3xpqZlZwLvZlZybnQm5mVnAu9mVnJ\nudCbmZWcC72ZWcm50JuZlVz+OqSXYWRjD5Nac+fX7lt1bWoewOiu9Ei6bsifR/+Zq/Lnfn/4RP66\ndO/8dv487acf/IvUvDWnbkjNA3jyhfvSM7vb8l+cex+M9EwOjE2PnHzDyfTMrs780jh14vH0zM01\n7ucjejOzknOhNzMrORd6M7OSc6E3Mys5F3ozs5JzoTczK7khMb2yp3cEHadzl9btvb0jNQ/glrb2\n9Mz2k1ekZ/7syKz0zB2/k7+kcEP+U8Tvb3o4NW9e657+d7pEv3bX1vTMV9/Mf86bDjWkZ/bmv4zY\nfXBieuaUSfnLKU9tOZaeWSsf0ZuZlZwLvZlZydVyzdhmSWslbZC0SdKfFu0TJa2StL34PKHqPo9L\n2iFpm6T76zkAMzPrWy1H9J3APRHxcWAusEDSHcBjwOqImAWsLr5G0hxgEXALsAD4rqT8k31mZlaT\nfgt9VJz7t1lj8RHAQmB50b4ceKjYXgg8ExGdEbET2AHMT+21mZnVrKZz9JIaJK0HDgKrImIN0BoR\n56ah7Adai+2pQPVUhb1Fm5mZDYKaCn1E9ETEXGAaMF/SrefdHlSO8msmaYmkdZLW9ZzIX33OzMwq\nLmkefUQck/QKlXPvByS1RUS7pDYqR/sA+4DpVXebVrSdn7UMWAYwZlZbjGnuvJz+X9SkMadS8wA2\n7pmWntk8On+Z4s6u/LdHqDN/gtYDd72envl3O25OzdvwXz+emgdwZmL+97KpNX9J4WhMj2T8LUfS\nM48cHpeeefTDMemZh5vzl2iuVS2zbiZLGl9sjwY+B2wFVgKLi90WAy8U2yuBRZJGSZoJzALWZnfc\nzMxqU8uhXxuwvJg5MwJYEREvSnoVWCHpEWA38DBARGyStILKmvjdwKMR0VOf7puZWX/6LfQRsRG4\n/QLtR4B7L3KfpcDSAffOzMwGzO+MNTMrORd6M7OSc6E3Myu5IbFMcYQ4czZ3Ltfb+9tS8wCunvZB\neuaR4y3pmVeOOZ2eedWtvzJDdsA6e/Nffi3J01UnvpS/TPHxz30sPfPUnfnTibsPjU7P/GB7/pLC\no47nH692Tsp/bY6Ycjg9s+bHHrRHNjOz/y9c6M3MSs6F3sys5FzozcxKzoXezKzkXOjNzErOhd7M\nrOSGxDz63p4RnDrRnJrZtD9/aIeOTUnPbOxQeub7bbnfSwA1569Lt/NQ/rLPow/lfj+P3Tc+NQ+g\n/b7u9MzmzflL4I649cP0zNE/zV9S+Pic/NfmyBP5x8BdPYN3RVUf0ZuZlZwLvZlZybnQm5mVnAu9\nmVnJudCbmZVcLdeMbZa0VtIGSZsk/WnR/g1J+yStLz4eqLrP45J2SNom6f56DsDMzPpWyxzETuCe\niOiQ1Aj8QtL/Km77TkT8WfXOkuYAi4BbgGuAlyXN9nVjzcwGRy3XjA2go/iysfiIPu6yEHgmIjqB\nnZJ2APOBVy96DwUNo3J/D8Ssk6l5ANGdPw+2szd/Hv2MtiPpmbu3Xp2eOW720fTMk10TUvNOzz2T\nmgfQEPnP+cj8SxDQcTT//RjjzvRVOi5PQ0f+GejuaZ3pmSe7mtIza1XTd0hSg6T1wEFgVUSsKW76\nqqSNkp6SdO4nbCpQfbWGvUWbmZkNgpoKfUT0RMRcYBowX9KtwPeA64G5QDvwrUt5YElLJK2TtK7n\nw/yjbzMzq7ikv3ki4hjwCrAgIg4UvwB6ge9TOT0DsA+YXnW3aUXb+VnLImJeRMxrGJd/OT0zM6uo\nZdbNZEnji+3RwOeArZKqL8r6ReCtYnslsEjSKEkzgVnA2txum5lZrWqZddMGLJfUQOUXw4qIeFHS\n/5A0l8o/ZncBXwGIiE2SVgCbgW7gUc+4MTMbPLXMutkI3H6B9i/3cZ+lwNKBdc3MzDKoMntykDsh\nHQJOAocHuy+JJuHxDHVlG5PHM7TVYzzXRcTk/nYaEoUeQNK6iJg32P3I4vEMfWUbk8cztA3meLzW\njZlZybnQm5mV3FAq9MsGuwPJPJ6hr2xj8niGtkEbz5A5R29mZvUxlI7ozcysDga90EtaUKxbv0PS\nY4Pdn1oUi7gdlPRWVdtESaskbS8+T6i6bUivzy9puqRXJG0urjnwtaJ9OI/pYtdRGLZjgn9cYPCX\nkl4svh6245G0S9KbxfUs1hVtw3Y8AJLGS/obSVslbZH0ySExpogYtA+gAXiHyuJoTcAGYM5g9qnG\nfn8G+ATwVlXbfwEeK7YfA/5zsT2nGNcoYGYx3obBHsN542kDPlFsjwPeLvo9nMckYGyx3QisAe4Y\nzmMq+vkfgB8BL5bgdbcLmHRe27AdT9HP5cC/K7abgPFDYUyDfUQ/H9gREe9GxFngGSrr2Q9pEfEz\n4IPzmhdSeZIpPj9U1f5MRHRGxE7g3Pr8Q0ZEtEfEG8X2h8AWKktLD+cxRURc6DoKw3ZMkqYB/xJ4\nsqp52I7nIobteCRdSeUg8AcAEXE2KgtBDvqYBrvQl2nt+taIaC+29wOtxfawGqOkGVSWvFjDMB/T\nRa6jMJzH9OfAHwK9VW3DeTxB5Qp0r0taUrQN5/HMBA4B/704vfakpBaGwJgGu9CXUlT+Lht205kk\njQWeBb4eESeqbxuOY4oLX0eh+vZhMyZJvwkcjIjXL7bPcBpP4VPF8/N54FFJn6m+cRiOZySVU7rf\ni4jbqSzr8k/+7zhYYxrsQl/T2vXDxIFzSzcXnw8W7cNijKpcD/hZ4IcR8VzRPKzHdE5UXUeB4Tum\nO4EvSNpF5RTnPZL+iuE7HiJiX/H5IPA8ldMWw3Y8VI7I98ZHV+D7GyqFf9DHNNiF/jVglqSZkpqo\nXFR85SD36XKtBBYX24uBF6rah/T6/JJE5bziloj4dtVNw3lMF7yOAsN0TBHxeERMi4gZVH5O/ndE\n/A7DdDySWiSNO7cN3EflmhbDcjwAEbEf2CPpY0XTvVSWax/8MQ2B/1I/QGWWxzvAHw12f2rs84+p\nXD6xi8pv8UeAq4DVwHbgZWBi1f5/VIxvG/D5we7/BcbzKSp/Tm4E1hcfDwzzMd0G/LIY01vAHxft\nw3ZMVf28i49m3QzL8VCZabeh+Nh07md/uI6nqo9zgXXF6+5vgQlDYUx+Z6yZWckN9qkbMzOrMxd6\nM7OSc6E3Mys5F3ozs5JzoTczKzkXejOzknOhNzMrORd6M7OS+398jqa8N1rX3gAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18e931cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(scipy.misc.imresize(classes_conv_out[0, 2].data.numpy(), (360,640), interp='nearest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 25, 16])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_conv_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool_size = (outputs.size(2), outputs.size(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3070  0.3327  0.3602\n",
       "[torch.FloatTensor of size 1x3]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 19, 10])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lrg_layers():\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((1,2)),\n",
    "        Convolution2D(8,3,3, border_mode='same'),\n",
    "        Dropout(p),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential (\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU (inplace)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU (inplace)\n",
       "  (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU (inplace)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU (inplace)\n",
       "  (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU (inplace)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU (inplace)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU (inplace)\n",
       "  (16): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU (inplace)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU (inplace)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU (inplace)\n",
       "  (23): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU (inplace)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU (inplace)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU (inplace)\n",
       "  (30): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_conv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1484\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/ClothingAttributeDataset/train/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sizes = [Image.open(filename).size for filename in glob(\"data/ClothingAttributeDataset/train/*.jpg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((267, 400), 'data/ClothingAttributeDataset/train/000001.jpg'),\n",
       " ((500, 751), 'data/ClothingAttributeDataset/train/000002.jpg'),\n",
       " ((266, 400), 'data/ClothingAttributeDataset/train/000003.jpg'),\n",
       " ((500, 750), 'data/ClothingAttributeDataset/train/000005.jpg'),\n",
       " ((500, 751), 'data/ClothingAttributeDataset/train/000006.jpg')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = [(Image.open(filename).size, filename) for filename in glob(\"data/ClothingAttributeDataset/train/*.jpg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Image.open(\"data/ClothingAttributeDataset/train/001619.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ClothingAttributeDataset/train/001619.jpg\n"
     ]
    }
   ],
   "source": [
    "for size, filename in images:\n",
    "    if size == (750, 543):\n",
    "        print(filename)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((266, 400), 434),\n",
       " ((500, 750), 254),\n",
       " ((267, 400), 215),\n",
       " ((500, 751), 184),\n",
       " ((750, 543), 68),\n",
       " ((563, 750), 48),\n",
       " ((501, 750), 17),\n",
       " ((266, 399), 15),\n",
       " ((500, 792), 13),\n",
       " ((700, 1050), 11)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, 266 * for k, v in Counter(sizes).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_sizes = [Image.open(filename).size for filename in glob(\"data/ClothingAttributeDataset/valid/*.jpg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((266, 400), 104),\n",
       " ((267, 400), 69),\n",
       " ((500, 750), 54),\n",
       " ((500, 751), 43),\n",
       " ((750, 543), 22),\n",
       " ((563, 750), 12),\n",
       " ((500, 792), 8),\n",
       " ((500, 818), 4),\n",
       " ((501, 750), 3),\n",
       " ((499, 750), 3),\n",
       " ((266, 399), 2),\n",
       " ((750, 500), 2),\n",
       " ((700, 1050), 2),\n",
       " ((751, 500), 2),\n",
       " ((526, 750), 2),\n",
       " ((500, 333), 2),\n",
       " ((750, 503), 2),\n",
       " ((681, 750), 1),\n",
       " ((638, 750), 1),\n",
       " ((500, 500), 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(valid_sizes).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
